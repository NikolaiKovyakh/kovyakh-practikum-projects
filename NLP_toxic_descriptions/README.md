## Викишоп. Определение токсичности комментариев.

Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. 


**Данные:** 
- набор данных с разметкой о токсичности правок.

**Цель**: 
- Обучить модель классифицировать комментарии как позитивные и негативные.
- Построить модель со значением метрики качества F1 не меньше 0.75. 

Необходимые библиотеки и их версии:
- *pandas* - версия 1.2.4 или новее
- *matplotlib* - версия 3.3.4 или новее
- *numpy* - версия 1.20.1 или новее
- *seaborn* - версия 0.11.1 или новее
- *nltk* - 3.6.1 или новее

**Выводы по проекту**:
На всех данных были получены следующие результаты с помощью модели Detoxify:
- F1 SCORE:  0.943
- ROC AUC SCORE:  0.998
- Precision: 0.948
- Recall: 0.938


Это очень высокая точность предсказания, которая поможет легко определить токсичный комментарий - и отправить его на модерацию, а также данная модель будет иметь очень малый процент ложных срабатываний.

 - Из всех комментариев, которые модель классифицировала как токсичные (TP + FP = 15639), найдено 15185 (TP) действительно токсичных. 836 классифицированы токсичными по ошибке (FP).
 - Из всех действительно токсичных (TP + FN = 16186), верно классифицировано 15185. 1001 токсичных комментариев были не замечены (FN)


Данные метрики и статистики были получены при пороге классификации = 0.45.
Если же необходимо повысить чувствительность классификатора, то порог необходимо будет понизить. Если же заказчик посчитает, что надо уменьшить количество ложных срабатываний, тогда надо сделать модель чуть менее строгой - и повысить порог классификации.

